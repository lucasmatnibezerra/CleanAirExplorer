{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ec9318a",
   "metadata": {},
   "source": [
    "# NASA Clean Air Challenge – Data Integration & API Pipeline\n",
    "\n",
    "Este notebook operacionaliza dados satelitais (TEMPO) e de superfície (OpenAQ, opcional AirNow / PGN / TOLNet) em endpoints web e tiles para o app React.\n",
    "\n",
    "**Escopo**: descobrir, subset, reprojetar, tilear TEMPO; cruzar com estações; gerar métricas de comparação; preparar forecast básico + caching.\n",
    "\n",
    "Legendas: (CORE) = essencial MVP, (OPT) = opcional valor extra.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f921880d",
   "metadata": {},
   "source": [
    "## 1. Setup Environment & Dependencies (CORE)\n",
    "Requisitos: Python >= 3.10\n",
    "\n",
    "Pacotes principais:\n",
    "- Autenticação & Acesso Cloud: `earthaccess`, `harmony-py`\n",
    "- Dados científicos: `xarray`, `rioxarray`, `numpy`, `pandas`, `scipy`\n",
    "- Geoespacial: `pyproj`, `rasterio`\n",
    "- API & Async: `fastapi`, `uvicorn`, `aiohttp`, `backoff`\n",
    "- Tiles & Imagens: `Pillow` (PIL), (opcional) `mercantile`, `pytiles`\n",
    "- ML & Métricas: `scikit-learn`\n",
    "- Cache: `redis` (cliente `redis`), fallback FS\n",
    "- Utilidades: `python-dotenv`, `requests`\n",
    "\n",
    "Observação: instalar via pip e congelar em `requirements.txt` para reprodutibilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0531425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional run) Install dependencies inside this environment\n",
    "# NOTE: Execute only once or manage via requirements.txt\n",
    "# %pip install earthaccess harmony-py xarray rioxarray pyproj rasterio numpy pandas scipy aiohttp fastapi uvicorn Pillow scikit-learn redis backoff python-dotenv requests mercantile\n",
    "import sys, platform\n",
    "print('Python', sys.version)\n",
    "assert sys.version_info >= (3,10), 'Python >=3.10 required'\n",
    "print('Platform', platform.platform())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91a2e0",
   "metadata": {},
   "source": [
    "## 2. Configure Earthdata Login Authentication (CORE)\n",
    "Use `.netrc` ou variáveis de ambiente:\n",
    "```\n",
    "machine urs.earthdata.nasa.gov login $EDL_USERNAME password $EDL_PASSWORD\n",
    "```\n",
    "Exemplo (interativo fallback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f0e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Optional\n",
    "try:\n",
    "    import earthaccess\n",
    "except ImportError:\n",
    "    earthaccess = None\n",
    "\n",
    "if earthaccess:\n",
    "    auth = earthaccess.login(strategy='netrc')  # fallback to interactive if no netrc\n",
    "    print('Authenticated:', auth)\n",
    "else:\n",
    "    print('earthaccess not installed in this environment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400cf55",
   "metadata": {},
   "source": [
    "## 3. Discover TEMPO Products via CMR/Harmony (CORE)\n",
    "Buscar coleções TEMPO NO₂ L3 (short_name ~ `TEMPO_NO2_L3*`). Filtrar por intervalo temporal + lat/lon bbox.\n",
    "\n",
    "Parametros chave:\n",
    "- temporal: `2025-10-01T00:00:00Z,2025-10-02T00:00:00Z`\n",
    "- bounding box: `minLon,minLat,maxLon,maxLat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1023bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, datetime as dt\n",
    "if earthaccess:\n",
    "    collections = earthaccess.search_data(identifier='TEMPO_NO2_L3*',\n",
    "                                          temporal=('2025-10-01','2025-10-02'))\n",
    "    print('Found collections:', len(collections))\n",
    "    if collections:\n",
    "        print(collections[0].umm.get('ShortName'))\n",
    "else:\n",
    "    print('Skip CMR search (earthaccess missing).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be85a185",
   "metadata": {},
   "source": [
    "## 4. Subset & Stream TEMPO NO₂ L3 Using Harmony (CORE)\n",
    "Enviar job Harmony com variáveis selecionadas e bbox. Poll até `Successful`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import harmony\n",
    "except ImportError:\n",
    "    harmony=None\n",
    "\n",
    "if harmony and earthaccess:\n",
    "    # Pseudocode (não executa sem collection real & params corretos)\n",
    "    from harmony import Client\n",
    "    hclient = Client()\n",
    "    request = hclient.submit(\n",
    "        collection_id='TEMPO_NO2_L3_SAMPLE',  # substituir por collection real\n",
    "        spatial={'bbox':(-49.0,-2.0,-48.0,-1.0)},\n",
    "        temporal={'start':'2025-10-01T00:00:00Z','end':'2025-10-01T02:00:00Z'},\n",
    "        variables=['NO2_TROPO_COLUMN']\n",
    "    )\n",
    "    print('Harmony Job ID:', request.job_id)\n",
    "else:\n",
    "    print('Harmony client not available; skipping job submit example.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb888a99",
   "metadata": {},
   "source": [
    "## 5. Read Cloud-Hosted Dataset into Xarray (CORE)\n",
    "Abrir NetCDF/Zarr remoto (HTTP ou S3) e inspecionar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "sample_url = os.getenv('TEMPO_SAMPLE_URL','')  # fornecer URL real\n",
    "if sample_url:\n",
    "    ds = xr.open_dataset(sample_url, engine='netcdf4', chunks={})\n",
    "    print(ds)\n",
    "    print('Variables:', list(ds.data_vars))\n",
    "else:\n",
    "    print('Defina TEMPO_SAMPLE_URL para abrir dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60ec2e",
   "metadata": {},
   "source": [
    "## 6. Reproject & Resample to Web Mercator (CORE)\n",
    "Usar `rioxarray` para set CRS (EPSG:4326) e reprojetar para `EPSG:3857`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6678b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import rioxarray  # noqa\n",
    "except ImportError:\n",
    "    rioxarray = None\n",
    "\n",
    "if 'ds' in globals():\n",
    "    var = list(ds.data_vars)[0] if ds.data_vars else None\n",
    "    if var:\n",
    "        da = ds[var]\n",
    "        if hasattr(da, 'rio'):  # ensure rioxarray\n",
    "            da = da.rio.write_crs('EPSG:4326')\n",
    "            merc = da.rio.reproject('EPSG:3857')\n",
    "            print('Reprojected shape:', merc.shape)\n",
    "        else:\n",
    "            print('rioxarray missing for reprojection.')\n",
    "    else:\n",
    "        print('No variable to reproject.')\n",
    "else:\n",
    "    print('Dataset not loaded; skip reprojection.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b10bdd",
   "metadata": {},
   "source": [
    "## 7. Generate AQI Color-Mapped Raster Tiles (CORE)\n",
    "Mapear valores normalizados para paleta (ex.: verde→amarelo→laranja→vermelho→roxo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f584c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def aqi_colorramp():\n",
    "    # return list of (threshold, (r,g,b)) sorted by threshold\n",
    "    return [\n",
    "        (0,(0,228,0)),(50,(255,255,0)),(100,(255,126,0)),(150,(255,0,0)),(200,(143,63,151)),(300,(126,0,35)),(500,(110,0,25))\n",
    "    ]\n",
    "\n",
    "def map_value_to_color(v):\n",
    "    ramp = aqi_colorramp()\n",
    "    for i,(th,_c) in enumerate(ramp):\n",
    "        if v <= th:\n",
    "            if i==0: return _c\n",
    "            prev_th, prev_c = ramp[i-1]\n",
    "            # linear blend\n",
    "            span = th - prev_th if th>prev_th else 1\n",
    "            t = (v - prev_th)/span\n",
    "            return tuple(int(prev_c[j] + t*( _c[j]-prev_c[j])) for j in range(3))\n",
    "    return ramp[-1][1]\n",
    "\n",
    "def render_tile(merc_da, z:int,x:int,y:int, vmin:float, vmax:float):\n",
    "    # Placeholder: would window merc_da by tile bounds; here random demo\n",
    "    size=256\n",
    "    data = np.random.uniform(vmin,vmax,(size,size))\n",
    "    arr = np.zeros((size,size,4), dtype=np.uint8)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            c = map_value_to_color(data[i,j])\n",
    "            arr[i,j,:3]=c; arr[i,j,3]=180\n",
    "    img = Image.fromarray(arr, mode='RGBA')\n",
    "    bio = BytesIO()\n",
    "    img.save(bio, format='PNG')\n",
    "    return bio.getvalue()\n",
    "\n",
    "print('Tile renderer ready (demo).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b033a2",
   "metadata": {},
   "source": [
    "## 8. FastAPI Tile Endpoint /tempo/tiles/{z}/{x}/{y}.png (CORE)\n",
    "Cache + render ou retornar imagem pré-gerada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb909e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import Response\n",
    "app = FastAPI(title='Clean Air API')\n",
    "\n",
    "@app.get('/tempo/tiles/{z}/{x}/{y}.png')\n",
    "async def tempo_tile(z:int,x:int,y:int, product:str='no2', time:str|None=None):\n",
    "    try:\n",
    "        data = render_tile(None,z,x,y,0,150)  # placeholder merc dataset\n",
    "        return Response(data, media_type='image/png', headers={'Cache-Control':'public, max-age=600'})\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "print('FastAPI app object created (not running uvicorn here).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca469a",
   "metadata": {},
   "source": [
    "## 9. Extract Point Time Series (CORE)\n",
    "Selecionar pixel mais próximo ou interpolar bilinear para lat/lon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_point_series(granule_datasets:list, lat:float, lon:float, var:str)->pd.DataFrame:\n",
    "    rows=[]\n",
    "    for ds in granule_datasets:\n",
    "        if var not in ds: continue\n",
    "        da = ds[var]\n",
    "        # very naive nearest index approach\n",
    "        if all(c in da.coords for c in ('lat','lon')):\n",
    "            lat_idx = abs(da['lat']-lat).argmin()\n",
    "            lon_idx = abs(da['lon']-lon).argmin()\n",
    "            val = da.isel(lat=lat_idx, lon=lon_idx).item()\n",
    "            ts = pd.to_datetime(ds.time.values[0]) if 'time' in ds.coords else None\n",
    "            rows.append({'ts':ts,'value':val})\n",
    "    return pd.DataFrame(rows).sort_values('ts')\n",
    "\n",
    "print('Point extraction helper defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85fd08",
   "metadata": {},
   "source": [
    "## 10. Fetch OpenAQ v3 Measurements (CORE)\n",
    "Uso de paginação e bbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25acf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, math\n",
    "\n",
    "def fetch_openaq(bbox:tuple[str,str,str,str], parameter:str, date_from:str, date_to:str, limit=1000, max_pages=3):\n",
    "    all_rows=[]\n",
    "    for page in range(1,max_pages+1):\n",
    "        url='https://api.openaq.org/v3/measurements'\n",
    "        params={\n",
    "            'parameter':parameter,\n",
    "            'date_from':date_from,\n",
    "            'date_to':date_to,\n",
    "            'bbox':','.join(bbox),\n",
    "            'limit':limit,\n",
    "            'page':page\n",
    "        }\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        for item in js.get('results', []):\n",
    "            all_rows.append({\n",
    "                'ts': item['date']['utc'],\n",
    "                'value': item['value'],\n",
    "                'unit': item['unit'],\n",
    "                'lat': item['coordinates']['latitude'] if item.get('coordinates') else None,\n",
    "                'lon': item['coordinates']['longitude'] if item.get('coordinates') else None\n",
    "            })\n",
    "        if len(js.get('results',[])) < limit:\n",
    "            break\n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "print('OpenAQ fetch helper ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1ccc3",
   "metadata": {},
   "source": [
    "## 11. Merge Satellite vs Ground (CORE)\n",
    "Alinhar por timestamp mais próximo (tolerância, ex: 30 min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b71531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_series(sat_df:pd.DataFrame, ground_df:pd.DataFrame, tolerance='30min'):\n",
    "    sat = sat_df.copy(); ground = ground_df.copy()\n",
    "    sat['ts'] = pd.to_datetime(sat['ts'])\n",
    "    ground['ts'] = pd.to_datetime(ground['ts'])\n",
    "    ground = ground.set_index('ts')\n",
    "    rows=[]\n",
    "    for _,r in sat.iterrows():\n",
    "        ts = r['ts']\n",
    "        window = ground.loc[ts-pd.Timedelta(tolerance): ts+pd.Timedelta(tolerance)]\n",
    "        if not window.empty:\n",
    "            g = window.iloc[(window.index-ts).abs().argmin()]\n",
    "            rows.append({'ts': ts, 'satellite_value': r['value'], 'ground_value': g['value']})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print('Alignment helper defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939e47c0",
   "metadata": {},
   "source": [
    "## 12. Comparison Metrics (CORE)\n",
    "RMSE, MAE, r (correlação de Pearson), Bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd58b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(df:pd.DataFrame):\n",
    "    df = df.dropna()\n",
    "    s = df['satellite_value'].to_numpy(); g = df['ground_value'].to_numpy()\n",
    "    if len(df)==0:\n",
    "        return {}\n",
    "    rmse = float(np.sqrt(((s-g)**2).mean()))\n",
    "    mae = float(np.abs(s-g).mean())\n",
    "    bias = float((s-g).mean())\n",
    "    r = float(np.corrcoef(s,g)[0,1]) if len(df)>1 else float('nan')\n",
    "    return {'rmse':rmse,'mae':mae,'bias':bias,'r':r,'n':int(len(df))}\n",
    "\n",
    "print('Metrics helper ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a01cf32",
   "metadata": {},
   "source": [
    "## 13. AQI Mapping (CORE)\n",
    "Função piecewise para NO₂ (exemplo simplificado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb9b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO2_BREAKS = [0,53,100,360,649,1249,2049]  # ppb-like example; adjust to standard\n",
    "AQI_BREAKS = [0,50,100,150,200,300,500]\n",
    "LABELS = ['Good','Moderate','USG','Unhealthy','Very Unhealthy','Hazardous']\n",
    "\n",
    "def aqi_from_no2(conc:float):\n",
    "    for i in range(1,len(NO2_BREAKS)):\n",
    "        if conc <= NO2_BREAKS[i]:\n",
    "            c_low, c_high = NO2_BREAKS[i-1], NO2_BREAKS[i]\n",
    "            a_low, a_high = AQI_BREAKS[i-1], AQI_BREAKS[i]\n",
    "            aqi = (a_high - a_low)/(c_high - c_low) * (conc - c_low) + a_low\n",
    "            return round(aqi), LABELS[i-1]\n",
    "    return 500, LABELS[-1]\n",
    "\n",
    "print('AQI conversion ready. Example:', aqi_from_no2(70))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c6b42",
   "metadata": {},
   "source": [
    "## 14. IMERG Precipitation (OPT)\n",
    "Subsetting similar ao TEMPO para variável `precipitationRate`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06965633",
   "metadata": {},
   "source": [
    "## 15. MERRA-2 PBL Height & Winds (OPT)\n",
    "Coleções: PBLH, U10M, V10M. Interpolar para grade TEMPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e1f0d",
   "metadata": {},
   "source": [
    "## 16. Feature Engineering para Forecast (CORE)\n",
    "Lags (1h,3h,6h), médias móveis, sin/cos hora do dia, vento (sqrt(u^2+v^2)), PBL normalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed08bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df:pd.DataFrame):\n",
    "    df = df.sort_values('ts').copy()\n",
    "    df['lag1'] = df['aqi'].shift(1)\n",
    "    df['lag3'] = df['aqi'].shift(3)\n",
    "    df['lag6'] = df['aqi'].shift(6)\n",
    "    df['roll3'] = df['aqi'].rolling(3).mean()\n",
    "    df['roll6'] = df['aqi'].rolling(6).mean()\n",
    "    df['hour'] = pd.to_datetime(df['ts']).dt.hour\n",
    "    df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "    df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "    return df\n",
    "print('Feature engineering helper defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eca3e1",
   "metadata": {},
   "source": [
    "## 17. Forecast Baseline (CORE)\n",
    "Persistência e regressão linear simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def train_linear_forecast(df:pd.DataFrame, horizon=48):\n",
    "    df_feat = add_features(df.dropna())\n",
    "    feature_cols = ['aqi','lag1','lag3','lag6','roll3','roll6','hour_sin','hour_cos']\n",
    "    df_feat = df_feat.dropna(subset=feature_cols)\n",
    "    X = df_feat[feature_cols].values\n",
    "    y = df_feat['aqi'].values\n",
    "    if len(X) < 10:\n",
    "        return []\n",
    "    model = LinearRegression().fit(X,y)\n",
    "    # naive iterative forecast\n",
    "    last = df_feat.iloc[-1].copy()\n",
    "    preds=[]\n",
    "    cur_time = pd.to_datetime(last['ts']) if 'ts' in last else pd.Timestamp.utcnow()\n",
    "    history = list(df_feat['aqi'].values)\n",
    "    for h in range(1,horizon+1):\n",
    "        cur_time += pd.Timedelta(hours=1)\n",
    "        hour = cur_time.hour\n",
    "        features = [history[-1], history[-1], history[-3] if len(history)>=3 else history[-1], history[-6] if len(history)>=6 else history[-1],\n",
    "                    np.mean(history[-3:]) if len(history)>=3 else history[-1], np.mean(history[-6:]) if len(history)>=6 else history[-1],\n",
    "                    np.sin(2*np.pi*hour/24), np.cos(2*np.pi*hour/24)]\n",
    "        pred = float(model.predict([features])[0])\n",
    "        history.append(pred)\n",
    "        aqi_val, label = aqi_from_no2(pred)  # reusando NO2 p/ demo; ajustar p/ conc real\n",
    "        preds.append({'ts': cur_time.isoformat(), 'aqi': aqi_val, 'aqi_label': label})\n",
    "    return preds\n",
    "\n",
    "print('Forecast trainer ready (demo).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cec93",
   "metadata": {},
   "source": [
    "## 18. /forecast Endpoint Schema (CORE)\n",
    "Resposta: `{location:{lat,lon}, horizonHours:48, points:[{ts,aqi,aqi_label}]}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7beb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get('/forecast')\n",
    "async def forecast_endpoint(lat:float, lon:float, horizon:int=48):\n",
    "    # placeholder historical series\n",
    "    hist = pd.DataFrame({'ts': pd.date_range(end=pd.Timestamp.utcnow(), periods=72, freq='H'), 'aqi': np.random.randint(10,120,72)})\n",
    "    preds = train_linear_forecast(hist, horizon=horizon)\n",
    "    return {'location': {'lat':lat,'lon':lon}, 'horizonHours': horizon, 'points': preds}\n",
    "\n",
    "print('Forecast endpoint stub added.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c64f2f",
   "metadata": {},
   "source": [
    "## 19. Caching Strategy (CORE)\n",
    "Chaves: `tempo:{product}:{time}:{z}:{x}:{y}` e `compare:{lat}:{lon}:{start}:{end}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc224ed",
   "metadata": {},
   "source": [
    "## 20. Async Orchestration & Rate Limiting (CORE)\n",
    "`aiohttp` + `asyncio.Semaphore` + `backoff` para OpenAQ / múltiplos granules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db0ecb",
   "metadata": {},
   "source": [
    "## 21. Frontend Contract Test Calls (CORE)\n",
    "Validar headers/imagens e JSON schema antes de integrar React."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc758ff",
   "metadata": {},
   "source": [
    "## 22. Environment & Secrets (.env) (CORE)\n",
    "Variáveis: `EDL_USERNAME`, `EDL_PASSWORD`, `REDIS_URL`, `CACHE_DIR`, `DEFAULT_BBOX`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde1d65",
   "metadata": {},
   "source": [
    "## 23. Provenance & Logging (CORE)\n",
    "Registrar: coleção, versão, granules, hora processamento, commit git, parâmetros de subset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
